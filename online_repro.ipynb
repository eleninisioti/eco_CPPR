{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EjoNYsdTRPuW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['FFMPEG_BINARY'] = 'ffmpeg'\n",
    "import moviepy.editor as mvp\n",
    "from moviepy.video.io.ffmpeg_writer import FFMPEG_VideoWriter\n",
    "from IPython.display import HTML, display, clear_output\n",
    "\n",
    "class VideoWriter:\n",
    "  def __init__(self, filename, fps=30.0, **kw):\n",
    "    self.writer = None\n",
    "    self.params = dict(filename=filename, fps=fps, **kw)\n",
    "\n",
    "  def add(self, img):\n",
    "    img = np.asarray(img)\n",
    "    if self.writer is None:\n",
    "      h, w = img.shape[:2]\n",
    "      self.writer = FFMPEG_VideoWriter(size=(w, h), **self.params)\n",
    "    if img.dtype in [np.float32, np.float64]:\n",
    "      img = np.uint8(img.clip(0, 1)*255)\n",
    "    if len(img.shape) == 2:\n",
    "      img = np.repeat(img[..., None], 3, -1)\n",
    "    self.writer.write_frame(img)\n",
    "\n",
    "  def close(self):\n",
    "    if self.writer:\n",
    "      self.writer.close()\n",
    "\n",
    "  def __enter__(self):\n",
    "    return self\n",
    "\n",
    "  def __exit__(self, *kw):\n",
    "    self.close()\n",
    "\n",
    "  def show(self, **kw):\n",
    "      self.close()\n",
    "      fn = self.params['filename']\n",
    "      display(mvp.ipython_display(fn, **kw))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGFLVeJeRdjW"
   },
   "source": [
    "## env abstract class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hG3xF5i-QtYr"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 The EvoJAX Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "from abc import ABC\n",
    "from abc import abstractmethod\n",
    "from typing import Tuple\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "\n",
    "class TaskState(ABC):\n",
    "    \"\"\"A template of the task state.\"\"\"\n",
    "    obs: jnp.ndarray\n",
    "\n",
    "\n",
    "class VectorizedTask(ABC):\n",
    "    \"\"\"Interface for all the EvoJAX tasks.\"\"\"\n",
    "\n",
    "    max_steps: int\n",
    "    obs_shape: Tuple\n",
    "    act_shape: Tuple\n",
    "    test: bool\n",
    "    multi_agent_training: bool = False\n",
    "\n",
    "    @abstractmethod\n",
    "    def reset(self, key: jnp.array) -> TaskState:\n",
    "        \"\"\"This resets the vectorized task.\n",
    "\n",
    "        Args:\n",
    "            key - A jax random key.\n",
    "        Returns:\n",
    "            TaskState. Initial task state.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @abstractmethod\n",
    "    def step(self,\n",
    "             state: TaskState,\n",
    "             action: jnp.ndarray) -> Tuple[TaskState, jnp.ndarray, jnp.ndarray]:\n",
    "        \"\"\"This steps once the simulation.\n",
    "\n",
    "        Args:\n",
    "            state - System internal states of shape (num_tasks, *).\n",
    "            action - Vectorized actions of shape (num_tasks, action_size).\n",
    "        Returns:\n",
    "            TaskState. Task states.\n",
    "            jnp.ndarray. Reward.\n",
    "            jnp.ndarray. Task termination flag: 1 for done, 0 otherwise.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reproduce here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen as nn\n",
    "\n",
    "import logging\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import itertools\n",
    "import functools\n",
    "\n",
    "from typing import Tuple, Callable, List, Optional, Iterable, Any\n",
    "from flax.struct import dataclass\n",
    "from evojax.task.base import TaskState\n",
    "from evojax.policy.base import PolicyNetwork\n",
    "from evojax.policy.base import PolicyState\n",
    "from evojax.util import create_logger\n",
    "from evojax.util import get_params_format_fn\n",
    "\n",
    "\n",
    "class MetaRNN_bcppr(nn.Module):\n",
    "    output_size: int\n",
    "    out_fn: str\n",
    "    hidden_layers: list\n",
    "    encoder_in: bool\n",
    "    encoder_layers: list\n",
    "\n",
    "    def setup(self):\n",
    "\n",
    "        self._num_micro_ticks = 1\n",
    "        self._lstm = nn.recurrent.LSTMCell()\n",
    "        self.convs = [nn.Conv(features=4, kernel_size=(3, 3),strides=2),nn.Conv(features=8, kernel_size=(3, 3),strides=2)]\n",
    "    \n",
    "\n",
    "        self._hiddens = [(nn.Dense(size)) for size in self.hidden_layers]\n",
    "        # self._encoder=nn.Dense(64)\n",
    "        self._output_proj = nn.Dense(self.output_size)\n",
    "        if (self.encoder_in):\n",
    "            self._encoder = [(nn.Dense(size)) for size in self.encoder_layers]\n",
    "\n",
    "    def __call__(self, h, c, inputs: jnp.ndarray, last_action: jnp.ndarray, reward: jnp.ndarray):\n",
    "        carry = (h, c)\n",
    "        # todo replace with scan\n",
    "        # inputs=self._encoder(inputs)\n",
    "        out = inputs\n",
    "        for conv in self.convs:\n",
    "          out=conv(out)\n",
    "          out = nn.relu(out)\n",
    "          out= nn.avg_pool(out, window_shape=(2, 2), strides=(1, 1))\n",
    "\n",
    "        out = jnp.ravel(out)\n",
    "\n",
    "        if (self.encoder_in):\n",
    "            for layer in self._encoder:\n",
    "                out = jax.nn.tanh(layer(out))\n",
    "\n",
    "        inputs_encoded = jnp.concatenate([out, last_action, reward])\n",
    "\n",
    "        for _ in range(self._num_micro_ticks):\n",
    "            carry, out = self._lstm(carry, inputs_encoded)\n",
    "        out=jnp.concatenate([inputs_encoded,out])\n",
    "        for layer in self._hiddens:\n",
    "            out = jax.nn.tanh(layer(out))\n",
    "        out = self._output_proj(out)\n",
    "\n",
    "        h, c = carry\n",
    "        if self.out_fn == 'tanh':\n",
    "            out = nn.tanh(out)\n",
    "        elif self.out_fn == 'softmax':\n",
    "            out = nn.softmax(out, axis=-1)\n",
    "        else:\n",
    "            if (self.out_fn != 'categorical'):\n",
    "                raise ValueError(\n",
    "                    'Unsupported output activation: {}'.format(self.out_fn))\n",
    "        return h, c, out\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class metaRNNPolicyState_bcppr(PolicyState):\n",
    "    lstm_h: jnp.array\n",
    "    lstm_c: jnp.array\n",
    "    keys: jnp.array\n",
    "\n",
    "\n",
    "class MetaRnnPolicy_bcppr(PolicyNetwork):\n",
    "\n",
    "    def __init__(self, input_dim: int,\n",
    "                 hidden_dim: int,\n",
    "                 output_dim: int,\n",
    "                 output_act_fn: str = \"categorical\",\n",
    "                 hidden_layers: list = [],\n",
    "                 encoder: bool = False,\n",
    "                 encoder_layers: list = [32, 32],\n",
    "                 logger: logging.Logger = None):\n",
    "\n",
    "        if logger is None:\n",
    "            self._logger = create_logger(name='MetaRNNolicy')\n",
    "        else:\n",
    "            self._logger = logger\n",
    "        model = MetaRNN_bcppr(output_dim, out_fn=output_act_fn, hidden_layers=hidden_layers, encoder_in=encoder,\n",
    "                              encoder_layers=encoder_layers)\n",
    "        self.params = model.init(jax.random.PRNGKey(0), jnp.zeros((hidden_dim)), jnp.zeros((hidden_dim)),\n",
    "                                 jnp.zeros(input_dim), jnp.zeros([output_dim]), jnp.zeros([1]))\n",
    "\n",
    "        self.num_params, format_params_fn = get_params_format_fn(self.params)\n",
    "        self._logger.info('MetaRNNPolicy.num_params = {}'.format(self.num_params))\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self._format_params_fn = jax.jit(jax.vmap(format_params_fn))\n",
    "        self._forward_fn = jax.jit(jax.vmap(model.apply))\n",
    "\n",
    "    def reset(self, states: TaskState) -> PolicyState:\n",
    "        \"\"\"Reset the policy.\n",
    "        Args:\n",
    "            TaskState - Initial observations.\n",
    "        Returns:\n",
    "            PolicyState. Policy internal states.\n",
    "        \"\"\"\n",
    "        keys = jax.random.split(jax.random.PRNGKey(0), states.obs.shape[0])\n",
    "        h = jnp.zeros((states.obs.shape[0], self.hidden_dim))\n",
    "        c = jnp.zeros((states.obs.shape[0], self.hidden_dim))\n",
    "        return metaRNNPolicyState_bcppr(keys=keys, lstm_h=h, lstm_c=c)\n",
    "    \n",
    "\n",
    "    def reset_b(self, obs: jnp.array) -> PolicyState:\n",
    "        \"\"\"Reset the policy.\n",
    "        Args:\n",
    "            TaskState - Initial observations.\n",
    "        Returns:\n",
    "            PolicyState. Policy internal states.\n",
    "        \"\"\"\n",
    "        keys = jax.random.split(jax.random.PRNGKey(0), obs.shape[0])\n",
    "        h = jnp.zeros((obs.shape[0], self.hidden_dim))\n",
    "        c = jnp.zeros((obs.shape[0], self.hidden_dim))\n",
    "        return metaRNNPolicyState_bcppr(keys=keys, lstm_h=h, lstm_c=c)\n",
    "\n",
    "    def get_actions(self, t_states: TaskState, params: jnp.ndarray, p_states: PolicyState):\n",
    "        params = self._format_params_fn(params)\n",
    "        h, c, out = self._forward_fn(params, p_states.lstm_h, p_states.lstm_c, t_states.obs, t_states.last_actions,\n",
    "                                     t_states.rewards)\n",
    "        return out, metaRNNPolicyState_bcppr(keys=p_states.keys, lstm_h=h, lstm_c=c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 The EvoJAX Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "from functools import partial\n",
    "from typing import Tuple\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "import numpy as np\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "from flax.struct import dataclass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "AGENT_VIEW=7\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AgentStates(object):\n",
    "    posx: jnp.uint16\n",
    "    posy: jnp.uint16\n",
    "    params:jnp.ndarray\n",
    "    policy_states:PolicyState\n",
    "    energy: jnp.ndarray\n",
    "    time_good_level:jnp.uint16\n",
    "    time_alive:jnp.uint16\n",
    "    time_under_level:jnp.uint16\n",
    "    alive:jnp.int8\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class State(TaskState):\n",
    "    obs: jnp.int8\n",
    "    last_actions:jnp.int8\n",
    "    rewards:jnp.int8\n",
    "    state: jnp.int8\n",
    "    agents: AgentStates\n",
    "    steps: jnp.int32\n",
    "    key: jnp.ndarray\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_ob(state: jnp.ndarray,pos_x:jnp.int32,pos_y:jnp.int32) -> jnp.ndarray:\n",
    "\n",
    "\n",
    "    obs=(jax.lax.dynamic_slice(jnp.pad(state,((AGENT_VIEW,AGENT_VIEW),(AGENT_VIEW,AGENT_VIEW),(0,0))),(pos_x-AGENT_VIEW+AGENT_VIEW,pos_y-AGENT_VIEW+AGENT_VIEW,0),(2*AGENT_VIEW+1,2*AGENT_VIEW+1,3)))\n",
    "    #obs=jnp.ravel(state)\n",
    "    \n",
    "    return obs\n",
    "\n",
    "def get_init_state_fn(key: jnp.ndarray,SX,SY,posx,posy,pos_food_x,pos_food_y) -> jnp.ndarray:\n",
    "    grid=jnp.zeros((SX,SY,4))    \n",
    "    grid=grid.at[posx,posy,0].add(1)\n",
    "    grid=grid.at[posx[:5],posy[:5],0].set(0)\n",
    "    grid=grid.at[pos_food_x,pos_food_y,1].set(1)\n",
    "    grid=grid.at[:,:,3].set(jnp.int8(jnp.clip(jnp.expand_dims((jnp.arange(0,SX))/30,1),0,127)))\n",
    "\n",
    "    #grid=grid.at[600:700,:300,3].set(0)\n",
    "    #grid=grid.at[:,:,3].set(5)\n",
    "\n",
    "    grid=grid.at[0,:,2].set(1)\n",
    "    grid=grid.at[-1,:,2].set(1)\n",
    "    grid=grid.at[:,0,2].set(1)\n",
    "    grid=grid.at[:,-1,2].set(1)\n",
    "    return (grid)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "get_obs_vector=jax.vmap(get_ob,in_axes=(None,0,0),out_axes=0)\n",
    "\n",
    "\n",
    "class Gridworld(VectorizedTask):\n",
    "    \"\"\"gridworld task.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 nb_agents: int =100,\n",
    "                 SX=300,\n",
    "                 SY=100,\n",
    "                 test: bool = False,\n",
    "                 energy_decay=0.05,\n",
    "                 max_age:int=1000,\n",
    "                 time_reproduce:int=150,\n",
    "                 time_death:int=40,\n",
    "                 max_ener=3.\n",
    "\n",
    "                 \n",
    "                 ):\n",
    "\n",
    "\n",
    "        self.obs_shape = (7,7,4)\n",
    "        #self.obs_shape=11*5*4\n",
    "        self.act_shape = tuple([5, ])\n",
    "        self.test = test\n",
    "        self.nb_agents=nb_agents\n",
    "        self.SX=SX\n",
    "        self.SY=SY\n",
    "        self.energy_decay=energy_decay\n",
    "        self.model=MetaRnnPolicy_bcppr(input_dim= ((AGENT_VIEW*2+1),(AGENT_VIEW*2+1),3),hidden_dim= 4,output_dim=5,encoder_layers=[],hidden_layers=[8])\n",
    "\n",
    "        self.energy_decay=energy_decay\n",
    "        self.max_age=max_age\n",
    "        self.time_reproduce=time_reproduce\n",
    "        self.time_death=time_death\n",
    "        self.max_ener=max_ener\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        def reset_fn(key):\n",
    "            next_key, key = random.split(key)\n",
    "            posx= random.randint(next_key,(self.nb_agents,),1,(SX-1))\n",
    "            next_key, key = random.split(key)\n",
    "            posy= random.randint(next_key,(self.nb_agents,),1,(SY-1))\n",
    "            next_key, key = random.split(key)\n",
    "            \n",
    "\n",
    "\n",
    "            pos_food_x= random.randint(next_key,(40*self.nb_agents,),1,(SX-1))\n",
    "            next_key, key = random.split(key)\n",
    "            pos_food_y= random.randint(next_key,(40*self.nb_agents,),1,(SY-1))\n",
    "            next_key, key = random.split(key)\n",
    "            grid=get_init_state_fn(key,SX,SY,posx,posy,pos_food_x,pos_food_y)\n",
    "\n",
    "\n",
    "            next_key, key = random.split(key)\n",
    "\n",
    "            params = jax.random.normal(\n",
    "                        next_key,\n",
    "                        (self.nb_agents,self.model.num_params,),\n",
    "                    )/100\n",
    "\n",
    "\n",
    "            policy_states=self.model.reset_b(jnp.zeros(self.nb_agents,))\n",
    "\n",
    "            \n",
    "\n",
    "            agents=AgentStates(posx=posx,posy=posy,energy=self.max_ener*jnp.ones((self.nb_agents,)).at[0:5].set(0),time_good_level=jnp.zeros((self.nb_agents,),dtype=jnp.uint16),params=params,policy_states=policy_states,\n",
    "                               time_alive=jnp.zeros((self.nb_agents,),dtype=jnp.uint16),time_under_level=jnp.zeros((self.nb_agents,),dtype=jnp.uint16),alive=jnp.ones((self.nb_agents,),dtype=jnp.uint16).at[0:self.nb_agents//2].set(0))\n",
    "            \n",
    "            return State(state=grid, obs=get_obs_vector(grid,posx,posy),last_actions=jnp.zeros((self.nb_agents,5)),rewards=jnp.zeros((self.nb_agents,1)),agents=agents,\n",
    "                         steps=jnp.zeros((), dtype=int), key=next_key)\n",
    "        self._reset_fn = jax.jit(reset_fn)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        def reproduce(params,posx,posy,energy,time_good_level,key,policy_states,time_alive,alive):\n",
    "            # use agent 0 to 4 as a dump always dead if no dead put in there to be sure not overiding the alive ones\n",
    "            # but maybe better to just make sure that there are 5 places available by checking if 5 dead (but this way may be better if we augment the 5)\n",
    "            dead=1-alive            \n",
    "            dead=dead.at[0:5].set(0.001)\n",
    "\n",
    "\n",
    "            next_key, key = random.split(key)\n",
    "            #empty_spots for new agent are dead ones\n",
    "            empty_spots=jax.random.choice(next_key,jnp.arange(time_good_level.shape[0]),p=dead,replace=False,shape=(5,))\n",
    "            \n",
    "            \n",
    "            #compute reproducer spot\n",
    "            next_key, key = random.split(key)\n",
    "            reproducer=jnp.where(time_good_level>self.time_reproduce,1,0)\n",
    "            reproducer=reproducer.at[0:5].set(0.001)\n",
    "            reproducer_spots=jax.random.choice(next_key,jnp.arange(time_good_level.shape[0]),p=reproducer/(reproducer.sum()+1e-10),replace=False,shape=(5,))\n",
    "            \n",
    "            \n",
    "            \n",
    "            next_key, key = random.split(key)\n",
    "            params=state.agents.params\n",
    "            #new agents params with mutate , and also take pos of parents \n",
    "            params=params.at[empty_spots].set(params[reproducer_spots]+0.02*jax.random.normal(next_key,(5,params.shape[1])))\n",
    "            posx=posx.at[empty_spots].set(posx[reproducer_spots])\n",
    "            posy=posy.at[empty_spots].set(posy[reproducer_spots])\n",
    "            \n",
    "            # new agents energy set at max\n",
    "            \n",
    "            # multiply by reproducer to be sure that the one that got selected by reproducer spot were reproducer indeed, \n",
    "            #in case nb reproducer <5 but again maybe we can just check that at least 5 reproducer but weird\n",
    "            energy=energy.at[empty_spots].set(self.max_ener*reproducer[reproducer_spots])\n",
    "            energy=energy.at[0:5].set(0.)\n",
    "            \n",
    "            # new agents alive and time alive , time_good_alive, and RNN state set at 0\n",
    "            \n",
    "            alive=alive.at[empty_spots].set(1*reproducer[reproducer_spots])\n",
    "            time_alive=time_alive.at[empty_spots].set(0)\n",
    "            time_good_level=time_good_level.at[empty_spots].set(0)\n",
    "            policy_states=metaRNNPolicyState_bcppr(lstm_h=policy_states.lstm_h.at[empty_spots].set(jnp.zeros(policy_states.lstm_h.shape[1])),lstm_c=policy_states.lstm_c.at[empty_spots].set(jnp.zeros(policy_states.lstm_c.shape[1])),keys=policy_states.keys)\n",
    "            \n",
    "            # put time good level of reproducer back to 0\n",
    "            #if in the dump don't put to 0 so that they can try reproduce in the next timestep\n",
    "            time_good_level=time_good_level.at[reproducer_spots].set(time_good_level[reproducer_spots]*(empty_spots<5))\n",
    "            \n",
    "            #kill the dump\n",
    "            alive=alive.at[0:5].set(0)\n",
    "            \n",
    "            \n",
    "\n",
    " \n",
    "            return(params,posx,posy,energy,time_good_level,policy_states,time_alive,alive)\n",
    "\n",
    "\n",
    "\n",
    "        def step_fn(state):\n",
    "\n",
    "            key=state.key\n",
    "            next_key, key = random.split(key)\n",
    "            \n",
    "            #model selection of action\n",
    "            actions_logit,policy_states=self.model.get_actions(state,state.agents.params,state.agents.policy_states)\n",
    "            actions=jax.nn.one_hot(jax.random.categorical(next_key,actions_logit*50,axis=-1),5)\n",
    "\n",
    " \n",
    "            \n",
    "            \n",
    "            grid=state.state\n",
    "            energy=state.agents.energy\n",
    "            alive=state.agents.alive\n",
    "\n",
    "            #move agent\n",
    "            action_int=actions.astype(jnp.int32)\n",
    "            posx=state.agents.posx-action_int[:,1]+action_int[:,3]\n",
    "            posy=state.agents.posy-action_int[:,2]+action_int[:,4]\n",
    "            \n",
    "            #wall\n",
    "            hit_wall=state.state[posx,posy,2]>0\n",
    "            posx=jnp.where(hit_wall,state.agents.posx,posx)\n",
    "            posy=jnp.where(hit_wall,state.agents.posy,posy)\n",
    "\n",
    "\n",
    "            posx=jnp.clip(posx,0,SX-1)\n",
    "            posy=jnp.clip(posy,0,SY-1)\n",
    "            grid=grid.at[state.agents.posx,state.agents.posy,0].set(0)\n",
    "            #add only the alive\n",
    "            grid=grid.at[posx,posy,0].add(1*(alive>0))\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "           \n",
    "\n",
    "            ### collect food\n",
    "\n",
    "            rewards=(alive>0)*(grid[posx,posy,1]>0)*(1/(grid[posx,posy,0]+1e-10))\n",
    "            grid=grid.at[posx,posy,1].add(-1*(alive>0))\n",
    "            grid =grid.at[:,:,1].set(jnp.clip(grid[:,:,1],0,1))\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "            #regrow \n",
    "\n",
    "            probability=jax.scipy.signal.convolve2d(grid[:,:,1],jnp.array([[0,1,0],[1,0,1],[0,1,0]])/4,mode=\"same\")\n",
    "            overcrowded=jax.scipy.signal.convolve2d(grid[:,:,1],np.ones((3,3)),mode=\"same\")\n",
    "            probability=probability*(overcrowded<5)\n",
    "            #modulate the probability with the climate value \n",
    "            probability=probability*jnp.clip(grid[:,:,3]/2000-grid[:,:,2],0,1)\n",
    "            next_key, key = random.split(key)\n",
    "            grid=grid.at[:,:,1].add(random.bernoulli(next_key,probability))\n",
    "\n",
    "            ####\n",
    "            steps = state.steps + 1\n",
    "            \n",
    "            #decay of energy and clipping\n",
    "            energy=energy-self.energy_decay+rewards\n",
    "            energy=jnp.clip(energy,-1000,self.max_ener)\n",
    "\n",
    "            time_good_level=jnp.where(energy>0,(state.agents.time_good_level+1)*alive,0)\n",
    "            time_under_level=jnp.where(energy<0,state.agents.time_under_level+1,0)\n",
    "\n",
    "\n",
    "            \n",
    "       \n",
    "            time_alive=state.agents.time_alive\n",
    "        \n",
    "            #look if still aliv\n",
    "            alive=jnp.where(jnp.logical_or(time_alive>self.max_age,time_under_level>self.time_death),0,alive)\n",
    "\n",
    "            time_alive=time_alive+alive\n",
    "            \n",
    "            # compute reproducer and go through the function only if there is one \n",
    "            reproducer=jnp.where(state.agents.time_good_level>self.time_reproduce,1,0)\n",
    "            next_key, key = random.split(key)\n",
    "            params,posx,posy,energy,time_good_level,policy_states,time_alive,alive=jax.lax.cond(reproducer.sum()>0,reproduce,lambda y,z,a,b,c,d,e,f,g: (y,z,a,b,c,e,f,g),*(state.agents.params,posx,posy,energy,time_good_level,next_key,state.agents.policy_states,time_alive,alive))\n",
    "            \n",
    " \n",
    "\n",
    "\n",
    "            done=False\n",
    "            steps = jnp.where(done, jnp.zeros((), jnp.int32), steps)\n",
    "            cur_state=State(state=grid, obs=get_obs_vector(grid,posx,posy),last_actions=actions,rewards=jnp.expand_dims(rewards,-1),\n",
    "                            agents=AgentStates(posx=posx,posy=posy,energy=energy,time_good_level=time_good_level,params=state.agents.params,policy_states=policy_states,\n",
    "                                               time_alive=time_alive,time_under_level=time_under_level,alive=alive),\n",
    "                        steps=steps, key=key)\n",
    "            #keep it in case we let agent several trials\n",
    "            state = jax.lax.cond(\n",
    "                done, lambda x: reset_fn(state.key), lambda x: x, cur_state)\n",
    "\n",
    "            return state\n",
    "        self._step_fn = jax.jit(step_fn)\n",
    "\n",
    "    def reset(self, key: jnp.ndarray) -> State:\n",
    "        return self._reset_fn(key)\n",
    "\n",
    "    def step(self,\n",
    "             state: State,\n",
    "             ) -> Tuple[State, jnp.ndarray, jnp.ndarray]:\n",
    "        return self._step_fn(state)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time without video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_a=time.time()\n",
    "nb_agents=1000\n",
    "env=Gridworld(SX=400,SY=200,nb_agents=nb_agents,max_age=900)\n",
    "key=jax.random.PRNGKey(np.random.randint(42))\n",
    "next_key, key = random.split(key)\n",
    "\n",
    "#reset_key=jax.random.split(next_key,nb_agents)\n",
    "state=env.reset(next_key)\n",
    "\n",
    "\n",
    "for i in range(10000):\n",
    "    if(i%200==0):\n",
    "        print(i,time.time()-time_a)\n",
    "          \n",
    "        state=env.step(state)\n",
    "\n",
    "\n",
    "\n",
    "print(time.time()-time_a)\n",
    "                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## time for video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_a=time.time()\n",
    "nb_agents=1000\n",
    "env=Gridworld(SX=400,SY=200,nb_agents=nb_agents,max_age=1000)\n",
    "key=jax.random.PRNGKey(np.random.randint(42))\n",
    "next_key, key = random.split(key)\n",
    "\n",
    "#reset_key=jax.random.split(next_key,nb_agents)\n",
    "state=env.reset(next_key)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with VideoWriter(\"out.mp4\",20.0) as vid:\n",
    "    for i in range(1000):\n",
    "\n",
    "\n",
    "          state=env.step(state)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "              #print(\"d\")\n",
    "          rgb_im=state.state[:,:,:3]\n",
    "          rgb_im=np.repeat(rgb_im,2,axis=0)\n",
    "          rgb_im=np.repeat(rgb_im,2,axis=1)\n",
    "          vid.add(rgb_im)\n",
    "\n",
    "\n",
    "\n",
    "vid.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(time.time()-time_a)\n",
    "                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_a=time.time()\n",
    "nb_agents=1000\n",
    "env=Gridworld(SX=400,SY=200,nb_agents=nb_agents,max_age=1000)\n",
    "key=jax.random.PRNGKey(np.random.randint(42))\n",
    "next_key, key = random.split(key)\n",
    "\n",
    "#reset_key=jax.random.split(next_key,nb_agents)\n",
    "state=env.reset(next_key)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for j in range(10000):\n",
    "    if(j%100==0):\n",
    "        with VideoWriter(\"out.mp4\",20.0) as vid:\n",
    "            for i in range(1000):\n",
    "\n",
    "\n",
    "                  state=env.step(state)\n",
    "\n",
    "                \n",
    "                \n",
    "                  \n",
    "\n",
    "                      #print(\"d\")\n",
    "                  rgb_im=state.state[:,:,:3]\n",
    "                  rgb_im=np.repeat(rgb_im,2,axis=0)\n",
    "                  rgb_im=np.repeat(rgb_im,2,axis=1)\n",
    "                  vid.add(rgb_im)\n",
    "\n",
    "\n",
    "\n",
    "        vid.show()\n",
    "    else:\n",
    "        for i in range(1000):          \n",
    "            state=env.step(state)\n",
    "\n",
    "\n",
    "\n",
    "print(time.time()-time_a)\n",
    "                                 "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "IGFLVeJeRdjW",
    "mpa_0DRBLb30",
    "jLCDjKU4QXbQ",
    "KUjvzcVnYRmb"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
